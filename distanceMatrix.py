# -*- coding: utf-8 -*-
"""TBCdI Matriz de Distâncias.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iiv6rc-ONQblv2cwJSBbQILfydHj_7Ww
"""

# Gabriel Marchioro Klein
#
#Sua tarefa será gerar uma matriz de distância, computando o cosseno do ângulo entre todos os 
#vetores que encontramos usando o tf-idf. Para isso use a seguinte fórmula para o cálculo do cosseno 
#use  a  fórmula  apresentada  em  Word2Vector  (frankalcantara.com) 
#(https://frankalcantara.com/Aulas/Nlp/out/Aula4.html#/0/4/2) e apresentada na figura a seguir:  

from bs4 import BeautifulSoup
import requests
import spacy
import re
import numpy as np

spacyEnglish = spacy.load("en_core_web_sm")
spacyEnglish.add_pipe('sentencizer')

allCorpus = []

links = [
  "https://en.wikipedia.org/wiki/Natural_language_processing", 
  "https://www.tableau.com/learn/articles/natural-language-processing-examples",
  "https://www.geeksforgeeks.org/natural-language-processing-overview/",
  "https://monkeylearn.com/natural-language-processing/",
  "https://cloud.google.com/learn/what-is-natural-language-processing"
]

def createCorpus(link):
    try:
      page = requests.get(link)
      soup = BeautifulSoup(page.content, 'html.parser')
    except requests.exceptions.MissingSchema:
      pass
    corpus = []
    for tag in soup.find_all(["h1", "h2", "h3", "h4", "h5", "h6", "p", "li"]):
      text = tag.get_text()
      strippedText = text.strip("\n")
      strippedText = text.strip()
      if len(strippedText) > 0:
        spacyText = spacyEnglish(strippedText)
        for sentence in spacyText.sents:
          corpus.append(sentence.text)
          allCorpus.append(sentence.text)

    return allCorpus

allVocab = []

def bagOfWords(allsentences):
    vocab = []                                                                    #Variável para armazenar as palavras do vocabulário
    ignore = ['a', "the", "is"]                                                   #Variável para ignorar stopwords
    #Criação do vocabulário:

    for sentence in allsentences:                                                 #Loop que separa todas as palavras das frases
        words = re.sub("[^\w]", " ",  sentence).split()                           #  separa as palavras das frases usando regex
        lowerText = [w.lower() for w in words if w not in ignore]                 #  transforma as palavras em lowercase
        vocab.extend(lowerText)                                                   #  adiciona as palavras ao vocabulário
    vocab = sorted(list(set(vocab)))                                              #Ordena as palavras em ordem alfabética
    docWords = [vocab]                                                            #Coloca o vocabulário na variável que vai conter o vetor de vetores
    totalWords = np.zeros(len(vocab))                                             #Variável que vai conter quantas palavras do vocabulário estão presentes em todo o documento
    for sentence in allsentences:                                                 #Loop que separa todas as palavras das frases
        words = re.sub("[^\w]", " ",  sentence).split()                           #  separa as palavras das frases usando regex
        lowerText = [w.lower() for w in words if w not in ignore]                 #
        wordBag = np.zeros(len(vocab))                                            #  cria um vetor vazio com zeros correspondentes ao tamanho do vocab
        for w in lowerText:                                                       #Loop que coloca as palavras na BoW
            for i, word in enumerate(vocab):                                      #  
                if word == w:                                                     # Checa qual é a palavra atual
                    wordBag[i] += 1                                               # Adiciona uma ocorrência no BoW
        totalWords = np.add(wordBag, totalWords)                                  #Vetor com a soma total da BoW de todas as frases
        docWords.append(wordBag)                                                  #Coloca o wordBag atual no vetor final

    for word in docWords[0]:
      if word in allVocab:
        pass
      else:
        allVocab.append(word)
    docWords.pop(0)
    bowFinal = np.array(docWords)
    return bowFinal

from cmath import log

def TF(bow):
  tf = np.zeros((len(bow), len(bow[0])))
  currentSentence = 0
  currentWord = 0
  allTerms = np.sum(bow)
  for sentence in bow.T:  
    for term in sentence:
      termFreq = term / allTerms
      tf[currentSentence, currentWord] = termFreq
      if currentWord == (bow.shape[1] - 1):
        currentWord = 0
      else:
        currentWord += 1
    if currentSentence == (bow.shape[0] - 1):
      currentSentence == 0
    else:
      currentSentence += 1
  return tf

def IDF(bow):
  idfs = []
  for word in range(len(allVocab)):
    wordCount = 0
    for sentenceRates in bow:
      if sentenceRates[word] > 0: 
        wordCount += 1
    idfs.append(np.log10(len(bow)/wordCount))
  return idfs

def TFIDF(tf, idf, bow):
  tfidf = np.zeros((len(bow),len(bow[0])))
  for i in range(len(tfidf)):
    for j in range(len(tfidf)):
      tfidf[i, j] = tf[i, j] * idf[j]
  return tfidf

allCorpus = []
for link in links:
  createCorpus(link)
bow = bagOfWords(allCorpus)
tf = TF(bow)
idf = IDF(bow)
tfidf = TFIDF(tf, idf, bow)

print(f'tfidf:\n {tfidf}')

def cosSim(a, b):
  cossim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
  return cossim

def distanceMatrix(arr):
  arrT = arr.T
  distanceArr = np.zeros((len(arrT),len(arrT)))
  for i in range(len(arrT)):
    for j in range(len(arrT)):
      if np.dot(arrT[i], arrT[j]) == 0:     #Por algum motivo, o numpy considera dividir o número zero como tendo resultado NaN, por isso, precisamos colocar esse if para garantir que não teremos NaNs na matriz final
         distanceArr[i, j] = 0
      else:
        distanceArr[i, j] = cosSim(arrT[i], arrT[j]) 
  return distanceArr

disMatrix = distanceMatrix(tfidf)

print(disMatrix)